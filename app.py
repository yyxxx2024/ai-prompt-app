import streamlit as st
from openai import OpenAI
import base64

# 1. é¡µé¢è®¾ç½®
st.set_page_config(page_title="ğŸ¨ AI æç¤ºè¯é­”æ³•å¸ˆ Pro", page_icon="ğŸ—ï¸", layout="centered")
st.title("âœ¨ AI æç¤ºè¯é­”æ³•å¸ˆ Pro")

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•° ---
def encode_image(uploaded_file):
    return base64.b64encode(uploaded_file.getvalue()).decode('utf-8')

# --- ğŸ” èº«ä»½éªŒè¯é€»è¾‘ ---
if "cached_api_key" not in st.session_state:
    st.session_state.cached_api_key = None

with st.sidebar:
    st.header("ğŸ” èº«ä»½éªŒè¯")
    SYSTEM_PASSWORD = st.secrets.get("APP_PASSWORD", None)
    SYSTEM_API_KEY = st.secrets.get("API_KEY", None)
    
    user_password = st.text_input("ğŸ”‘ è®¿é—®å¯†ç ", type="password", placeholder="è¾“å…¥å¯†ç ï¼Œè‡ªåŠ¨åŠ è½½ Key")
    manual_key = st.text_input("æˆ–æ‰‹åŠ¨è¾“å…¥ Key", type="password", label_visibility="collapsed")

    if SYSTEM_PASSWORD and user_password == SYSTEM_PASSWORD:
        if SYSTEM_API_KEY:
            st.session_state.cached_api_key = SYSTEM_API_KEY
            st.success("âœ… å¯†ç æ­£ç¡®ï¼ä»¤ç‰Œå·²é”å®š")
        else:
            st.error("âš ï¸ äº‘ç«¯ Secrets æœªé…ç½® API_KEY")
    elif manual_key:
        st.session_state.cached_api_key = manual_key
    elif user_password and user_password != SYSTEM_PASSWORD:
        st.error("âŒ å¯†ç é”™è¯¯")

    st.markdown("---")
    st.header("âš™ï¸ API è®¾ç½®")
    base_url = st.text_input("API åœ°å€", value="https://hk-api.gptbest.vip/v1")
    st.caption("ğŸ“ æ–‡æœ¬æ¨¡å‹ (DeepSeek)")
    text_model = st.text_input("Text Model", value="deepseek-chat", label_visibility="collapsed")
    st.caption("ğŸ–¼ï¸ å›¾ç‰‡æ¨¡å‹ (GPT-4o)")
    vision_model = st.text_input("Vision Model", value="gpt-4o-mini", label_visibility="collapsed")

# --- ğŸ—ï¸ åŠŸèƒ½æ ‡ç­¾é¡µ ---
tab1, tab2 = st.tabs(["ğŸ“ æ–‡æœ¬ç”Ÿæˆæç¤ºè¯", "ğŸ–¼ï¸ å›¾ç‰‡åæ¨æç¤ºè¯ (çœ‹å›¾)"])

# ==========================================
# ğŸ‘‰ æ ‡ç­¾ 1ï¼šæ–‡æœ¬ç”Ÿæˆ (å»ºç­‘å¢å¼ºç‰ˆ)
# ==========================================
with tab1:
    st.subheader("âœï¸ æè¿°ç”»é¢ï¼Œç”Ÿæˆ Prompt")
    user_input = st.text_area("ä½ æƒ³ç”»ä»€ä¹ˆï¼Ÿ", height=100, placeholder="ä¾‹å¦‚ï¼šä¸€ä¸ªåè½åœ¨æ‚¬å´–è¾¹çš„ç¾æœ¯é¦†ï¼Œæ‰å“ˆé£æ ¼...")

    c1, c2 = st.columns(2)
    with c1: 
        ratio = st.selectbox("ç”»å¹…", ["--ar 16:9 (æ¨ªå±)", "--ar 4:3 (æ ‡å‡†)", "--ar 3:2 (æ‘„å½±)", "--ar 9:16 (æ‰‹æœº)", "--ar 1:1 (æ–¹å½¢)"])
    with c2: 
        # ğŸ‘‡ æ ¸å¿ƒï¼šå¢åŠ äº†â€œå»ºç­‘æ•ˆæœå›¾ä¸“ç”¨â€
        mode = st.selectbox("æ¨¡å¼", [
            "ğŸ—ï¸ å»ºç­‘æ•ˆæœå›¾ä¸“ç”¨ (ArchViz)", 
            "æ ‡å‡†æ¨¡å¼ (MJ/SD)", 
            "è‡ªç„¶è¯­è¨€ (Google)", 
            "äºŒæ¬¡å…ƒ (Niji)", 
            "å†™å®æ‘„å½±", 
            "3Dæ¸²æŸ“"
        ])

    # âœ¨âœ¨âœ¨ åŠ¨æ€é«˜çº§é€‰é¡¹ (æ ¹æ®æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢) âœ¨âœ¨âœ¨
    with st.expander("ğŸ¨ ç‚¹å‡»å±•å¼€ï¼šé«˜çº§å‚æ•°é…ç½® (å·²æ ¹æ®æ¨¡å¼è‡ªåŠ¨ä¼˜åŒ–)"):
        
        # ğŸ”´ æƒ…å†µ Aï¼šå¦‚æœæ˜¯ã€å»ºç­‘æ¨¡å¼ã€‘ï¼Œæ˜¾ç¤ºå»ºç­‘å¸ˆä¸“ç”¨çš„ 6 å¤§å‚æ•°
        if "å»ºç­‘" in mode:
            st.caption("ğŸ—ï¸ **å»ºç­‘å¸ˆä¸“ç”¨å‚æ•°é¢æ¿**")
            ac1, ac2, ac3 = st.columns(3)
            with ac1:
                arch_view = st.selectbox("ğŸ“ æ„å›¾è§†ç‚¹", ["ä¸æŒ‡å®š", "äººè§†è§’åº¦ (Eye Level)", "åŠé¸Ÿç° (Semi-Aerial)", "é¡¶è§†å›¾/æ€»å¹³ (Top View)", "è™«è§†/ä»°è§† (Worm's Eye)", "ä¸€ç‚¹é€è§† (One Point)", "è½´æµ‹å›¾ (Isometric)"])
            with ac2:
                arch_time = st.selectbox("ğŸŒ¤ï¸ å¤©æ°”æ—¶åˆ»", ["ä¸æŒ‡å®š", "é»„é‡‘æ—¶åˆ» (Golden Hour)", "è“è°ƒæ—¶åˆ» (Blue Hour)", "æ­£åˆæ™´å¤© (Sunny Noon)", "é˜´å¤©æ¼«å°„ (Overcast)", "é›¨å¤œ (Rainy Night)", "é›¾å¤© (Foggy)"])
            with ac3:
                arch_env = st.selectbox("ğŸŒ³ å‘¨è¾¹ç¯å¢ƒ", ["ä¸æŒ‡å®š", "ç¹åè¡—é“ (Busy Street)", "æ£®æ—æ™¯è§‚ (Forest)", "æµ·æ»¨/æ°´å²¸ (Waterfront)", "é›ªæ™¯ (Snowy)", "è’æ¼  (Desert)", "æç®€æ£šæ‹ (Studio)"])
            
            ac4, ac5, ac6 = st.columns(3)
            with ac4:
                arch_style = st.selectbox("ğŸ›ï¸ å»ºç­‘é£æ ¼", ["ä¸æŒ‡å®š", "ç°ä»£æç®€ (Minimalist)", "å‚æ•°åŒ–è®¾è®¡ (Parametric)", "ç²—é‡ä¸»ä¹‰ (Brutalist)", "å·¥ä¸šé£ (Industrial)", "æœªæ¥ä¸»ä¹‰ (Futuristic)", "ä¼ ç»Ÿä¸­å¼ (Traditional Chinese)"])
            with ac5:
                arch_mat = st.selectbox("ğŸ§¶ ä¸»ä½“æè´¨", ["ä¸æŒ‡å®š", "æ¸…æ°´æ··å‡åœŸ (Concrete)", "ç»ç’ƒå¹•å¢™ (Glass Facade)", "æœ¨æ ¼æ … (Wooden Louvers)", "ç™½å¢™ (White Stucco)", "çº¢ç – (Red Brick)", "è€å€™é’¢ (Corten Steel)"])
            with ac6:
                arch_render = st.selectbox("ğŸ–¥ï¸ æ¸²æŸ“å¼•æ“é£æ ¼", ["ä¸æŒ‡å®š", "V-Ray é€¼çœŸæ¸²æŸ“", "Unreal Engine 5", "Lumion é£æ ¼", "æ‰‹ç»˜è‰å›¾ (Sketch)", "æ°´å½©æ¸²æŸ“ (Watercolor)", "æ¨¡å‹é£ (Maquette)"])

        # ğŸ”µ æƒ…å†µ Bï¼šå¦‚æœæ˜¯å…¶ä»–æ¨¡å¼ï¼Œæ˜¾ç¤ºé€šç”¨çš„å‚æ•°
        else:
            st.caption("ğŸ¨ **é€šç”¨é«˜çº§å‚æ•°**")
            col_a, col_b, col_c = st.columns(3)
            with col_a: 
                lighting = st.selectbox("ğŸ’¡ å…‰çº¿", ["ä¸æŒ‡å®š", "è‡ªç„¶å…‰", "ç”µå½±å…‰", "é»„é‡‘æ—¶åˆ»", "éœ“è™¹å…‰", "æŸ”å…‰"])
            with col_b: 
                camera = st.selectbox("ğŸ“· è§†è§’", ["ä¸æŒ‡å®š", "å¹¿è§’", "å¾®è·", "é¸Ÿç°", "å¹³è§†", "é±¼çœ¼"])
            with col_c: 
                mood = st.selectbox("ğŸ­ æ°›å›´", ["ä¸æŒ‡å®š", "æ¢¦å¹»", "å²è¯—", "é˜´éƒ", "å®é™", "æ´»åŠ›"])

        st.markdown("---")
        st.caption("ğŸ›ï¸ **Midjourney å‚æ•°å¾®è°ƒ**")
        m1, m2 = st.columns(2)
        with m1: stylize = st.slider("é£æ ¼åŒ– (--s)", 0, 1000, 250)
        with m2: chaos = st.slider("å¤šæ ·æ€§ (--c)", 0, 100, 0)
        
        negative_prompt = st.text_input("ğŸš« è´Ÿé¢æç¤ºè¯", value="text, watermark, blurry, low quality, bad anatomy, ugly, distorted structures")

    # ğŸ”¥ æ ¸å¿ƒæŒ‡ä»¤
    base_instruction = """
    You are an expert AI prompt engineer.
    IMPORTANT: Output exactly two parts:
    CN: [Optimized Chinese description]
    EN: [Final English prompt]
    """
    
    # æ¨¡å¼å¯¹åº”çš„ Prompt é€»è¾‘
    mode_rules = {
        "ğŸ—ï¸ å»ºç­‘æ•ˆæœå›¾ä¸“ç”¨ (ArchViz)": """
        For EN: You are an Architectural Visualization Expert. 
        1. Use professional ArchViz terminology (e.g., 'curtain wall', 'cantilever', 'volumetric lighting').
        2. Target style: ArchDaily, Dezeen, Behance high-end rendering.
        3. Format: Comma-separated tags. 
        4. Always add: '8k resolution, photorealistic, architectural photography, highly detailed'.
        """,
        "æ ‡å‡†æ¨¡å¼ (MJ/SD)": "For EN: Output comma-separated keywords. Visual descriptors.",
        "è‡ªç„¶è¯­è¨€ (Google)": "For EN: Write a rich, descriptive English paragraph.",
        "äºŒæ¬¡å…ƒ (Niji)": "For EN: Anime style, cel shading, vibrant colors.",
        "å†™å®æ‘„å½±": "For EN: Photorealistic, 8k, shot on Sony A7RIV.",
        "3Dæ¸²æŸ“": "For EN: 3D render, blender, c4d, octane render."
    }
    
    sys_prompt = base_instruction + mode_rules.get(mode, mode_rules["æ ‡å‡†æ¨¡å¼ (MJ/SD)"])

    if st.button("ğŸš€ ç”ŸæˆåŒè¯­æç¤ºè¯", type="primary"):
        current_key = st.session_state.cached_api_key
        if not current_key: st.error("ğŸš¨ è¯·å…ˆè¾“å…¥å¯†ç ï¼"); st.stop()
        
        try:
            client = OpenAI(api_key=current_key, base_url=base_url)
            
            # ğŸ‘‡ æ™ºèƒ½æ‹¼æ¥å‚æ•°ï¼ˆæ ¹æ®æ¨¡å¼ä¸åŒï¼Œæ‹¼æ¥ä¸åŒçš„å˜é‡ï¼‰
            details = []
            
            if "å»ºç­‘" in mode:
                # æ‹¼æ¥å»ºç­‘ä¸“ç”¨å‚æ•°
                if arch_view != "ä¸æŒ‡å®š": details.append(f"View: {arch_view}")
                if arch_time != "ä¸æŒ‡å®š": details.append(f"Time: {arch_time}")
                if arch_env != "ä¸æŒ‡å®š": details.append(f"Environment: {arch_env}")
                if arch_style != "ä¸æŒ‡å®š": details.append(f"Style: {arch_style}")
                if arch_mat != "ä¸æŒ‡å®š": details.append(f"Material: {arch_mat}")
                if arch_render != "ä¸æŒ‡å®š": details.append(f"Render Style: {arch_render}")
            else:
                # æ‹¼æ¥é€šç”¨å‚æ•°
                if lighting != "ä¸æŒ‡å®š": details.append(f"Lighting: {lighting}")
                if camera != "ä¸æŒ‡å®š": details.append(f"Camera: {camera}")
                if mood != "ä¸æŒ‡å®š": details.append(f"Mood: {mood}")
            
            full_req = f"User Request: {user_input}. Specific Requirements: {', '.join(details)}"

            with st.spinner('AI æ­£åœ¨ç»˜åˆ¶è“å›¾...'):
                resp = client.chat.completions.create(
                    model=text_model,
                    messages=[{"role": "system", "content": sys_prompt}, {"role": "user", "content": full_req}]
                )
                
                raw_content = resp.choices[0].message.content
                
                cn_text = "è§£æä¸­..."
                en_text = raw_content
                if "CN:" in raw_content and "EN:" in raw_content:
                    parts = raw_content.split("EN:")
                    cn_text = parts[0].replace("CN:", "").strip()
                    en_text = parts[1].strip()
                
                final_en = f"{en_text} {ratio.split(' ')[0]}"
                if "è‡ªç„¶è¯­è¨€" not in mode:
                    final_en += f" --s {stylize} --c {chaos}"
                    if negative_prompt: final_en += f" --no {negative_prompt}"

            st.markdown("### ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ„æ€")
            st.code(cn_text, language="text", wrap_lines=True)
            st.markdown("### ğŸ‡ºğŸ‡¸ è‹±æ–‡ Prompt (å¯ä»¥ç›´æ¥å¤åˆ¶)")
            st.code(final_en, language="text", wrap_lines=True)
            
        except Exception as e:
            st.error(f"å‡ºé”™ï¼š{str(e)}")

# ==========================================
# ğŸ‘‰ æ ‡ç­¾ 2ï¼šå›¾ç‰‡åæ¨ (ä¿æŒä¸å˜)
# ==========================================
with tab2:
    st.subheader("ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡ï¼Œåæ¨ Prompt")
    uploaded_file = st.file_uploader("æ‹–å…¥å‚è€ƒå›¾", type=["jpg", "png"])
    if uploaded_file and st.button("ğŸ” å¼€å§‹åæ¨"):
        current_key = st.session_state.cached_api_key
        if not current_key: st.error("è¯·å…ˆè¾“å…¥å¯†ç ï¼"); st.stop()
        try:
            client = OpenAI(api_key=current_key, base_url=base_url)
            img_b64 = encode_image(uploaded_file)
            with st.spinner('AI æ­£åœ¨çœ‹å›¾...'):
                resp = client.chat.completions.create(
                    model=vision_model,
                    messages=[{"role": "user", "content": [{"type": "text", "text": "åˆ†æè¿™å¼ å›¾ã€‚è¾“å‡ºæ ¼å¼ï¼š\nCN: [ä¸­æ–‡æè¿°]\nEN: [MJå…³é”®è¯]"}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}]}]
                )
            raw = resp.choices[0].message.content
            cn, en = raw, raw
            if "EN:" in raw: cn, en = raw.split("EN:")[0].replace("CN:", ""), raw.split("EN:")[1]
            c1, c2 = st.columns([1, 2])
            with c1: st.image(uploaded_file, width=150)
            with c2: st.code(cn.strip(), language="text", wrap_lines=True); st.code(en.strip(), language="text", wrap_lines=True)
        except Exception as e: st.error(f"å‡ºé”™ï¼š{e}")
